\chapter{GaitNet Training Configuration}
\label{chap:appendix-gaitnet-training-configuration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reward Function Analysis}
\label{sec:appendix-reward-function-analysis}

Designing the reward functions for training GaitNet (see
\autoref{tab:reward-function}) proved to be a challenging task.
Various combinations of rewards often resulted in undesirable
behaviors, such as the robot frequently lifting its feet while idle
or dragging feet during locomotion. Achieving a natural gait required
careful balancing, particularly between \textit{op\_reward} and
\textit{mdp.foot\_slip\_penalty}.

\begin{table}[h!]
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Function}\tablefootnote{Functions named "mdp.*" are
      built-in functions     provided by the NVIDIA Isaac Lab
    framework.} & \textbf{Weight} &
    \textbf{Description} \\
    \hline
    mdp.is\_alive & 0.4 & Reward for being alive \\
    mdp.track\_lin\_vel\_xy\_exp & 0.5 & Reward for tracking linear
    velocity in the XY plane \\     mdp.track\_ang\_vel\_z\_exp & 0.5
    & Reward for tracking angular     velocity around the Z axis \\
    \hline
    op\_reward & -0.1 & Penalty for performing actions \\
    mdp.is\_terminated & -200 & Penalty for termination \\
    mdp.lin\_vel\_z\_l2 & -2.5 & Penalty for linear velocity in the Z
    direction \\     mdp.ang\_vel\_xy\_l2 & -0.1 & Penalty for
    angular velocity in the     XY plane \\
    mdp.flat\_orientation\_l2 & -8 & Penalty for non-flat orientation
    \\     mdp.foot\_slip\_penalty & -6 & Penalty for foot slip \\
    \hline
  \end{tabular}
  \caption{Reward functions used to train GaitNet.}
  \label{tab:reward-function}
\end{table}

In addition to the rewards listed in Table \ref{tab:reward-function},
several other reward functions were explored but ultimately not used.

The \textit{a\_foot\_in\_swing} reward was intended to encourage the
agent to frequently lift its feet during early learning. However, it
proved unnecessary, as the initial network weights already favored
foot movement. Furthermore, including this reward caused the agent to
move its feet excessively when idle.

The \textit{no\_op\_reward} was designed to encourage the agent to
remain idle when no useful actions were available. In practice, this
reward required an excessively high weighting to have any effect, at
which point it would overshadow other rewards. The
\textit{op\_reward} was found to be a more effective alternative for
achieving the desired behavior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Termination Functions}
\label{sec:appendix-termination-functions}

The termination functions used in training GaitNet are summarized in
Table \ref{tab:termination-function}. These functions help define the
conditions under which an episode ends, either applying the
termination penalty or simply signaling an invalid state.

\begin{table}[h!]
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Function}\tablefootnote{Functions named "mdp.*" are
      built-in functions     provided by the NVIDIA Isaac Lab
    framework.} & \textbf{Time     Out}\tablefootnote{Time Out
      indicates whether the     termination applies the
    mdp.is\_terminated penalty.} &
    \textbf{Description} \\
    \hline
    mdp.time\_out & True & Terminate at the end of the episode \\
    mdp.bad\_orientation & False & Terminate if the robot's
    orientation is too far from upright \\
    mdp.root\_height\_below\_minimum & False & Terminate if the
    robot's base height is too low \\
    mdp.terrain\_out\_of\_bounds & True & Terminate if the robot
    leaves the terrain bounds \\     foot\_in\_void & False &
    Terminate if any foot steps into the void \\
    \hline
  \end{tabular}
  \caption{Termination functions used to train GaitNet.}
  \label{tab:termination-function}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Commands}
\label{sec:appendix-commands}

The command used in training GaitNet is summarized below. This
command defines the highest level input to the environment, $\mathbf u$.

\begin{itemize}
  \item \textbf{UniformVelocityCommand}: This command samples a desired
    velocity vector $\mathbf{v} = [v_x, v_y, \omega_z]$ from a
    uniform   distribution.
    \begin{itemize}
      \item resampling time range: (2.5, 10)\,s
      \item $v_x$ range: (-0.2, 0.2)\,m/s
      \item $v_y$ range: (-0.2, 0.2)\,m/s
      \item $\omega_z$ range: (-0.4, 0.4)\,rad/s
      \item probability of zero command: 0.05
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PPO Hyperparameters}
\label{sec:appendix-ppo-hyperparameters}

The PPO hyperparameters used for training GaitNet are summarized in
Table \ref{tab:ppo-hyperparams}. While not all parameters were
rigorously tuned, they were found to perform well in practice. The
discount factor \textit{gamma} was specifically selected in relation
to the agent observation frequency of 25,Hz to provide a reasonable
effective horizon. The \textit{learning\_rate} was chosen relatively
high to compensate for the slower data collection speed.

\begin{table}[h!]
  \centering
  \begin{tabular}{ll}
    \hline
    \textbf{Hyperparameter} & \textbf{Value} \\
    \hline
    clip\_param & 0.3 \\     num\_learning\_epochs & 8 \\
    num\_mini\_batches & 4 \\     value\_loss\_coef & 0.5 \\
    entropy\_coef & 0.02 \\     learning\_rate & 3e-4 \\
    max\_grad\_norm & 1.0 \\     use\_clipped\_value\_loss & True \\
    gamma & 0.99 \\     lam & 0.95 \\
    \hline
  \end{tabular}
  \caption{Hyperparameters used for PPO training.}
  \label{tab:ppo-hyperparams}
\end{table}
